% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ccc_repeated.R
\name{ccc_lmm_reml}
\alias{ccc_lmm_reml}
\title{Concordance Correlation via REML (Linear Mixed-Effects Model)}
\usage{
ccc_lmm_reml(
  data,
  response,
  rind,
  method = NULL,
  time = NULL,
  interaction = FALSE,
  max_iter = 100,
  tol = 1e-06,
  Dmat = NULL,
  Dmat_type = c("time-avg", "typical-visit", "weighted-avg", "weighted-sq"),
  Dmat_weights = NULL,
  Dmat_rescale = TRUE,
  ci = FALSE,
  conf_level = 0.95,
  ci_mode = c("auto", "raw", "logit"),
  verbose = FALSE,
  digits = 4,
  use_message = TRUE,
  ar = c("none", "ar1"),
  ar_rho = NA_real_,
  slope = c("none", "subject", "method", "custom"),
  slope_var = NULL,
  slope_Z = NULL,
  drop_zero_cols = TRUE,
  vc_select = c("auto", "none"),
  vc_alpha = 0.05,
  vc_test_order = c("subj_time", "subj_method"),
  include_subj_method = NULL,
  include_subj_time = NULL,
  sb_zero_tol = 1e-10
)
}
\arguments{
\item{data}{A data frame.}

\item{response}{Character. Response variable name.}

\item{rind}{Character. Subject ID variable name (random intercept).}

\item{method}{Character or \code{NULL}. Optional column name of method factor
(added to fixed effects).}

\item{time}{Character or \code{NULL}. Optional column name of time factor
(added to fixed effects).}

\item{interaction}{Logical. Include \code{method:time} interaction?
(default \code{FALSE}).}

\item{max_iter}{Integer. Maximum iterations for variance-component updates
(default \code{100}).}

\item{tol}{Numeric. Convergence tolerance on parameter change
(default \code{1e-6}).}

\item{Dmat}{Optional \eqn{n_t \times n_t} numeric matrix to weight/aggregate
time-specific fixed biases in the \eqn{S_B} quadratic form. If supplied, it
is used (after optional mass rescaling; see \code{Dmat_rescale}) whenever at
least two \emph{present} time levels exist; otherwise it is ignored. \strong{If
\code{Dmat} is \code{NULL}}, a canonical kernel \eqn{D_m} is \emph{constructed}
from \code{Dmat_type} and \code{Dmat_weights} (see below). \code{Dmat} should be
symmetric positive semidefinite; small asymmetries are symmetrized internally.}

\item{Dmat_type}{Character, one of \code{c("time-avg","typical-visit",
  "weighted-avg","weighted-sq")}. Only used when \code{Dmat = NULL}.
It selects the aggregation target for time-specific fixed biases in
\eqn{S_B}. Options are:

\itemize{
\item \code{"time-avg"}: square of the time-averaged bias, \eqn{D_m=(1/n_t)\,11^\top}.
\item \code{"typical-visit"}: average of squared per-time biases, \eqn{D_m=I_{n_t}}.
\item \code{"weighted-avg"}: square of a weighted average, \eqn{D_m=n_t\,w\,w^\top} with \eqn{\sum w=1}.
\item \code{"weighted-sq"}: weighted average of squared biases, \eqn{D_m=n_t\,\mathrm{diag}(w)} with \eqn{\sum w=1}.
}
Pick \code{"time-avg"} for CCC targeting the time-averaged measurement; pick
\code{"typical-visit"} for CCC targeting a randomly sampled visit (typical occasion).
Default \code{"time-avg"}.}

\item{Dmat_weights}{Optional numeric weights \eqn{w} used when
\code{Dmat_type \%in\% c("weighted-avg","weighted-sq")}. Must be nonnegative and
finite. If \code{names(w)} are provided, they should match the \emph{full} time
levels in \code{data}; they are aligned to the \emph{present} time subset per fit.
If unnamed, the length must equal the number of present time levels. In all cases
\eqn{w} is internally normalized to sum to 1.}

\item{Dmat_rescale}{Logical. When \code{TRUE} (default), the supplied/built
\eqn{D_m} is rescaled to satisfy the simple mass rule
\eqn{1^\top D_m 1 = n_t}. This keeps the \eqn{S_B} denominator invariant and
harmonizes with the \eqn{\kappa}-shrinkage used for variance terms.}

\item{ci}{Logical. If \code{TRUE}, return a CI container; limits are computed
by a large-sample delta method for CCC (see \strong{CIs} note below).}

\item{conf_level}{Numeric in \eqn{(0,1)}. Confidence level when
\code{ci = TRUE} (default \code{0.95}).}

\item{ci_mode}{Character scalar; one of \code{c("auto","raw","logit")}.
Controls how confidence intervals are computed when \code{ci = TRUE}.
If \code{"raw"}, a Wald CI is formed on the CCC scale and truncated to
\code{[0,1]}. If \code{"logit"}, a Wald CI is computed on the
\eqn{\mathrm{logit}(\mathrm{CCC})} scale and back-transformed to the
original scale (often more stable near 0 or 1). If \code{"auto"}
(default), the method is chosen per estimate based on simple diagnostics
(e.g., proximity to the \code{[0,1]} boundary / numerical stability),
typically preferring \code{"logit"} near the boundaries and \code{"raw"}
otherwise.}

\item{verbose}{Logical. If \code{TRUE}, prints a structured summary of the
fitted variance components and \eqn{S_B} for each fit. Default \code{FALSE}.}

\item{digits}{Integer \eqn{(\ge 0)}. Number of decimal places to use in the
printed summary when \code{verbose = TRUE}. Default \code{4}.}

\item{use_message}{Logical. When \code{verbose = TRUE}, choose the printing
mechanism, where \code{TRUE} uses \code{message()} (respects \code{sink()},
easily suppressible via \code{suppressMessages()}), whereas \code{FALSE}
uses \code{cat()} to \code{stdout}. Default \code{TRUE}.}

\item{ar}{Character. Residual correlation structure: \code{"none"} (iid) or
\code{"ar1"} for subject-level AR(1) correlation within contiguous time
runs. Default \code{c("none")}.}

\item{ar_rho}{Numeric in \eqn{(-0.999,\,0.999)} or \code{NA}.
If \code{ar = "ar1"} and \code{ar_rho} is finite, it is treated as fixed.
If \code{ar = "ar1"} and \code{ar_rho = NA}, \eqn{\rho} is estimated by
profiling a 1-D objective (REML when available; an approximation otherwise).
Default \code{NA_real_}.}

\item{slope}{Character. Optional extra random-effect design \eqn{Z}.
With \code{"subject"} a single random slope is added (one column in \eqn{Z});
with \code{"method"} one column per method level is added; with
\code{"custom"} you provide \code{slope_Z} directly. Default
\code{c("none","subject","method","custom")}.}

\item{slope_var}{For \code{slope \%in\% c("subject","method")}, a character
string giving the name of a column in \code{data} used as the slope regressor
(e.g., centered time). It is looked up inside \code{data}; do not
pass the vector itself. NAs are treated as zeros in \eqn{Z}.}

\item{slope_Z}{For \code{slope = "custom"}, a numeric matrix with \eqn{n}
rows (same order as \code{data}) providing the full extra random-effect
design \eqn{Z}. \strong{Each column of \code{slope_Z} has its own variance
component} \eqn{\sigma_{Z,j}^2}; columns are treated as \emph{uncorrelated}
(diagonal block in \eqn{G}). Ignored otherwise.}

\item{drop_zero_cols}{Logical. When \code{slope = "method"}, drop all-zero
columns of \eqn{Z} after subsetting (useful in pairwise fits). Default
\code{TRUE}.}

\item{vc_select}{Character scalar; one of \code{c("auto","none")}.
Controls how the subject by method \eqn{\sigma^2_{A\times M}} ("subj_method") and
subject by time \eqn{\sigma^2_{A\times T}} ("subj_time") variance components are
included. If \code{"auto"} (default), the function performs boundary-aware
REML likelihood-ratio tests (LRTs; null on the boundary at zero with a
half-\eqn{\chi^2_1} reference) to decide whether to retain each component,
in the order given by \code{vc_test_order}. If \code{"none"}, no testing
is done and inclusion is taken from \code{include_subj_method}/\code{include_subj_time}
(or, if \code{NULL}, from the mere presence of the corresponding factor in
the design).  In pairwise fits, the decision is made independently for each
method pair.}

\item{vc_alpha}{Numeric scalar in \eqn{(0,1)}; default \code{0.05}.
Per-component significance level for the boundary-aware REML LRTs used when
\code{vc_select = "auto"}. The tests are one-sided for variance components
on the boundary and are \emph{not} multiplicity-adjusted.}

\item{vc_test_order}{Character vector (length 2) with a permutation of
\code{c("subj_time","subj_method")}; default \code{c("subj_time","subj_method")}. Specifies the order
in which the two variance components are tested when \code{vc_select = "auto"}.
The component tested first may be dropped before testing the second. If a
factor is absent in the design (e.g., no time factor so "subj_time" is undefined),
the corresponding test is skipped.}

\item{include_subj_method, include_subj_time}{Logical scalars or \code{NULL}.
When \code{vc_select = "none"}, these control whether the
\eqn{\sigma^2_{A\times M}} ("subj_method") and \eqn{\sigma^2_{A\times T}} ("subj_time")
random effects are included (\code{TRUE}) or excluded (\code{FALSE}) in the model.
If \code{NULL} (default), inclusion defaults to the presence of the
corresponding factor in the data (i.e., at least two method/time levels).
When \code{vc_select = "auto"}, these arguments are ignored (automatic
selection is used instead).}

\item{sb_zero_tol}{Non-negative numeric scalar; default \code{1e-10}.
Numerical threshold for the fixed-effect dispersion term \eqn{S_B}.
After computing \eqn{\widehat{S_B}} and its delta-method variance, if
\eqn{\widehat{S_B} \le} \code{sb_zero_tol} or non-finite, the procedure
treats \eqn{S_B} as fixed at zero in the delta step. It sets
\eqn{d_{S_B}=0} and \eqn{\mathrm{Var}(\widehat{S_B})=0}, preventing
numerical blow-ups of \code{SE(CCC)} when \eqn{\widehat{S_B}\to 0} and the
fixed-effects variance is ill-conditioned for the contrast. This stabilises
inference in rare boundary cases; it has no effect when
\eqn{\widehat{S_B}} is comfortably above the threshold.}
}
\description{
Compute Lin's Concordance Correlation Coefficient (CCC) from a linear
mixed-effects model fitted by REML. The fixed-effects part can include
\code{method} and/or \code{time} (optionally their interaction), with a
subject-specific random intercept to capture between-subject variation.
Large \eqn{n \times n} inversions are avoided by solving small per-subject
systems.

\strong{Assumption:} time levels are treated as \emph{regular, equally spaced}
visits indexed by their order within subject. The AR(1) residual model is
in discrete time on the visit index (not calendar time). \code{NA} time codes
break the serial run. Gaps in the factor levels are \emph{ignored} (adjacent
observed visits are treated as lag-1).
}
\details{
For measurement \eqn{y_{ij}} on subject \eqn{i} under fixed
levels (method, time), we fit
\deqn{ y = X\beta + Zu + \varepsilon,\qquad
       u \sim N(0,\,G),\ \varepsilon \sim N(0,\,R). }
Notation: \eqn{m} subjects, \eqn{n=\sum_i n_i} total rows;
\eqn{nm} method levels; \eqn{nt} time levels; \eqn{q_Z} extra
random-slope columns (if any); \eqn{r=1+nm+nt} (or \eqn{1+nm+nt+q_Z} with slopes).
Here \eqn{Z} is the subject-structured random-effects design and \eqn{G} is
block-diagonal at the subject level with the following \emph{per-subject}
parameterisation. Specifically,
\itemize{
\item one random intercept with variance \eqn{\sigma_A^2};
\item optionally, \emph{method} deviations (one column per method level)
with a common variance \eqn{\sigma_{A\times M}^2} and zero
covariances across levels (i.e., multiple of an identity);
\item optionally, \emph{time} deviations (one column per time level)
with a common variance \eqn{\sigma_{A\times T}^2} and zero
covariances across levels;
\item optionally, an \emph{extra} random effect aligned with \eqn{Z}
(random slope), where each \emph{column} has its own variance
\eqn{\sigma_{Z,j}^2} and columns are uncorrelated.
}
The fixed-effects design is \code{~ 1 + method + time} and, if
\code{interaction=TRUE}, \code{+ method:time}.

\strong{Residual correlation \eqn{R} (regular, equally spaced time).}
Write \eqn{R_i=\sigma_E^2\,C_i(\rho)}. With \code{ar="none"}, \eqn{C_i=I}.
With \code{ar="ar1"}, within-subject residuals follow a \emph{discrete} AR(1)
process along the visit index after sorting by increasing time level. Ties
retain input order, and any \code{NA} time code breaks the series so each
contiguous block of non-\code{NA} times forms a run. The correlation
between \emph{adjacent observed visits} in a run is \eqn{\rho}; we do not use
calendar-time gaps. Internally we work with the \emph{precision} of the AR(1)
correlation: for a run of length \eqn{L\ge 2}, the tridiagonal inverse has
\deqn{ (C^{-1})_{11}=(C^{-1})_{LL}=\frac{1}{1-\rho^2},\quad
       (C^{-1})_{tt}=\frac{1+\rho^2}{1-\rho^2}\ (2\le t\le L-1),\quad
       (C^{-1})_{t,t+1}=(C^{-1})_{t+1,t}=\frac{-\rho}{1-\rho^2}. }
The working inverse is \eqn{\,R_i^{-1}=\sigma_E^{-2}\,C_i(\rho)^{-1}}.

\strong{Per-subject Woodbury system.} For subject \eqn{i} with \eqn{n_i}
rows, define the per-subject random-effects design \eqn{U_i} (columns:
intercept, method indicators, time indicators; dimension
\eqn{\,r=1+nm+nt\,}). The core never forms
\eqn{V_i = R_i + U_i G U_i^\top} explicitly. Instead,
\deqn{ M_i \;=\; G^{-1} \;+\; U_i^\top R_i^{-1} U_i, }
and accumulates GLS blocks via rank-\eqn{r} corrections using
\eqn{\,V_i^{-1} = R_i^{-1}-R_i^{-1}U_i M_i^{-1}U_i^\top R_i^{-1}\,}:
\deqn{ X^\top V^{-1} X \;=\; \sum_i \Big[
       X_i^\top R_i^{-1}X_i \;-\; (X_i^\top R_i^{-1}U_i)\,
       M_i^{-1}\,(U_i^\top R_i^{-1}X_i) \Big], }
\deqn{ X^\top V^{-1} y \;=\; \sum_i \Big[
       X_i^\top R_i^{-1}y_i \;-\; (X_i^\top R_i^{-1}U_i)\,M_i^{-1}\,
       (U_i^\top R_i^{-1}y_i) \Big]. }
Because \eqn{G^{-1}} is diagonal with positive entries, each \eqn{M_i} is
symmetric positive definite; solves/inversions use symmetric-PD routines with
a small diagonal ridge and a pseudo-inverse if needed.

\strong{Random-slope \eqn{Z}.}
Besides \eqn{U_i}, the function can include an extra design \eqn{Z_i}.
\itemize{
\item \code{slope="subject"}: \eqn{Z} has one column (the regressor in
\code{slope_var}); \eqn{Z_{i}} is the subject-\eqn{i} block, with its own
variance \eqn{\sigma_{Z,1}^2}.
\item \code{slope="method"}: \eqn{Z} has one column per method level;
row \eqn{t} uses the slope regressor if its method equals level \eqn{\ell},
otherwise 0; all-zero columns can be dropped via
\code{drop_zero_cols=TRUE} after subsetting. Each column has its own
variance \eqn{\sigma_{Z,\ell}^2}.
\item \code{slope="custom"}: \eqn{Z} is provided fully via \code{slope_Z}.
Each column is an independent random effect with its own variance
\eqn{\sigma_{Z,j}^2}; cross-covariances among columns are set to 0.
}
Computations simply augment \eqn{\tilde U_i=[U_i\ Z_i]} and the corresponding
inverse-variance block. The EM updates then include, for each column \eqn{j=1,\ldots,q_Z},
\deqn{ \sigma_{Z,j}^{2\,(new)} \;=\; \frac{1}{m}
      \sum_{i=1}^m \Big( b_{i,\text{extra},j}^2 +
      (M_i^{-1})_{\text{extra},jj} \Big)
      \quad (\text{if } q_Z>0). }
\emph{Interpretation:} the \eqn{\sigma_{Z,j}^2} represent additional within-subject
variability explained by the slope regressor(s) in column \eqn{j} and are \emph{not} part of the CCC
denominator (agreement across methods/time).

\strong{EM-style variance-component updates.} With current \eqn{\hat\beta},
form residuals \eqn{r_i = y_i - X_i\hat\beta}. The BLUPs and conditional
covariances are
\deqn{ b_i \;=\; M_i^{-1}\,(U_i^\top R_i^{-1} r_i), \qquad
      \mathrm{Var}(b_i\mid y) \;=\; M_i^{-1}. }
Let \eqn{e_i=r_i-U_i b_i}. Expected squares then yield closed-form updates:
\deqn{ \sigma_A^{2\,(new)} \;=\; \frac{1}{m}\sum_i \Big( b_{i,0}^2 +
(M_i^{-1})_{00} \Big), }
\deqn{ \sigma_{A\times M}^{2\,(new)} \;=\; \frac{1}{m\,nm}
      \sum_i \sum_{\ell=1}^{nm}\!\Big( b_{i,\ell}^2 +
      (M_i^{-1})_{\ell\ell} \Big)
      \quad (\text{if } nm>0), }
\deqn{ \sigma_{A\times T}^{2\,(new)} \;=\; \frac{1}{m\,nt}
      \sum_i \sum_{t=1}^{nt}
      \Big( b_{i,t}^2 + (M_i^{-1})_{tt} \Big)
      \quad (\text{if } nt>0), }
\deqn{ \sigma_E^{2\,(new)} \;=\; \frac{1}{n} \sum_i
      \Big( e_i^\top C_i(\rho)^{-1} e_i \;+\;
      \mathrm{tr}\!\big(M_i^{-1}U_i^\top C_i(\rho)^{-1} U_i\big) \Big), }
together with the per-column update for \eqn{\sigma_{Z,j}^2} given above.
Iterate until the \eqn{\ell_1} change across components is \eqn{<}\code{tol}
or \code{max_iter} is reached.

\strong{Fixed-effect dispersion \eqn{S_B}: choosing the time-kernel \eqn{D_m}.}

Let \eqn{d = L^\top \hat\beta} stack the within-time, pairwise method differences,
grouped by time as \eqn{d=(d_1^\top,\ldots,d_{n_t}^\top)^\top} with
\eqn{d_t \in \mathbb{R}^{P}} and \eqn{P = n_m(n_m-1)}. The symmetric
positive semidefinite kernel \eqn{D_m \succeq 0} selects which functional of the
bias profile \eqn{t \mapsto d_t} is targeted by \eqn{S_B}. Internally, the code
rescales any supplied/built \eqn{D_m} to satisfy \eqn{1^\top D_m 1 = n_t} for
stability and comparability.

\itemize{
\item \code{Dmat_type = "time-avg"} (square of the time-averaged bias).
Let \deqn{ w \;=\; \frac{1}{n_t}\,\mathbf{1}_{n_t}, \qquad
               D_m \;\propto\; I_P \otimes (w\,w^\top), }
so that
\deqn{ d^\top D_m d \;\propto\; \sum_{p=1}^{P}
           \left( \frac{1}{n_t}\sum_{t=1}^{n_t} d_{t,p} \right)^{\!2}. }
Methods have equal \eqn{\textit{time-averaged}}
means within subject, i.e. \eqn{\sum_{t=1}^{n_t} d_{t,p}/n_t = 0} for all
\eqn{p}. Appropriate when decisions depend on an average over time and
opposite-signed biases are allowed to cancel.

\item \code{Dmat_type = "typical-visit"} (average of squared per-time biases).
With equal visit probability, take
\deqn{ D_m \;\propto\; I_P \otimes
           \mathrm{diag}\!\Big(\tfrac{1}{n_t},\ldots,\tfrac{1}{n_t}\Big), }
yielding
\deqn{ d^\top D_m d \;\propto\; \frac{1}{n_t}
           \sum_{t=1}^{n_t}\sum_{p=1}^{P} d_{t,p}^{\,2}. }
Methods agree on a \eqn{\textit{typical}}
occasion drawn uniformly from the visit set. Use when each visit matters
on its own; alternating signs \eqn{d_{t,p}} do not cancel.

\item \code{Dmat_type = "weighted-avg"} (square of a weighted time average).
For user weights \eqn{a=(a_1,\ldots,a_{n_t})^\top} with \eqn{a_t \ge 0}, set
\deqn{ w \;=\; \frac{a}{\sum_{t=1}^{n_t} a_t}, \qquad
           D_m \;\propto\; I_P \otimes (w\,w^\top), }
so that
\deqn{ d^\top D_m d \;\propto\; \sum_{p=1}^{P}
           \left( \sum_{t=1}^{n_t} w_t\, d_{t,p} \right)^{\!2}. }
Methods have equal \eqn{\textit{weighted
    time-averaged}} means, i.e. \eqn{\sum_{t=1}^{n_t} w_t\, d_{t,p} = 0} for all
\eqn{p}. Use when some visits (e.g., baseline/harvest) are a priori more
influential; opposite-signed biases may cancel according to \eqn{w}.

\item \code{Dmat_type = "weighted-sq"} (weighted average of squared per-time biases).
With the same weights \eqn{w}, take
\deqn{ D_m \;\propto\; I_P \otimes \mathrm{diag}(w_1,\ldots,w_{n_t}), }
giving
\deqn{ d^\top D_m d \;\propto\; \sum_{t=1}^{n_t} w_t
           \sum_{p=1}^{P} d_{t,p}^{\,2}. }
Methods agree at visits sampled with
probabilities \eqn{\{w_t\}}, counting each visit’s discrepancy on its own.
Use when per-visit agreement is required but some visits should be
emphasised more than others.
}

\strong{Time-averaging for CCC (regular visits).}
The reported CCC targets agreement of the \emph{time-averaged} measurements
per method within subject by default (\code{Dmat_type="time-avg"}). Averaging over \eqn{T}
non-\code{NA} visits shrinks time-varying components by
\deqn{ \kappa_T^{(g)} \;=\; 1/T, \qquad
      \kappa_T^{(e)} \;=\; \{T + 2\sum_{k=1}^{T-1}(T-k)\rho^k\}/T^2, }
with \eqn{\kappa_T^{(e)}=1/T} when residuals are i.i.d. With unbalanced \eqn{T}, the
implementation averages the per-(subject,method) \eqn{\kappa} values across the
pairs contributing to CCC and then clamps \eqn{\kappa_T^{(e)}} to
\eqn{[10^{-12},\,1]} for numerical stability. Choosing
\code{Dmat_type="typical-visit"} makes \eqn{S_B} match the interpretation of a
randomly sampled occasion instead.

\strong{Concordance correlation coefficient.} The CCC used is
\deqn{ \mathrm{CCC} \;=\;
      \frac{\sigma_A^2 + \kappa_T^{(g)}\,\sigma_{A\times T}^2}
           {\sigma_A^2 + \sigma_{A\times M}^2 +
            \kappa_T^{(g)}\,\sigma_{A\times T}^2 + S_B +
            \kappa_T^{(e)}\,\sigma_E^2}. }
Special cases: with no method factor, \eqn{S_B=\sigma_{A\times M}^2=0}; with
a single time level, \eqn{\sigma_{A\times T}^2=0} (no \eqn{\kappa}-shrinkage).
When \eqn{T=1} or \eqn{\rho=0}, both \eqn{\kappa}-factors equal 1. The \emph{extra}
random-effect variances \eqn{\{\sigma_{Z,j}^2\}} (if used) are \emph{not} included.

\strong{CIs / SEs (delta method for CCC).}
Let
\deqn{ \theta \;=\; \big(\sigma_A^2,\ \sigma_{A\times M}^2,\
\sigma_{A\times T}^2,\ \sigma_E^2,\ S_B\big)^\top, }
and write \eqn{\mathrm{CCC}(\theta)=N/D} with
\eqn{N=\sigma_A^2+\kappa_T^{(g)}\sigma_{A\times T}^2} and
\eqn{D=\sigma_A^2+\sigma_{A\times M}^2+\kappa_T^{(g)}\sigma_{A\times T}^2+S_B+\kappa_T^{(e)}\sigma_E^2}.
The gradient components are
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_A^2}
      \;=\; \frac{\sigma_{A\times M}^2 + S_B + \kappa_T^{(e)}\sigma_E^2}{D^2}, }
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_{A\times M}^2}
      \;=\; -\,\frac{N}{D^2}, \qquad
       \frac{\partial\,\mathrm{CCC}}{\partial \sigma_{A\times T}^2}
      \;=\; \frac{\kappa_T^{(g)}\big(\sigma_{A\times M}^2 + S_B +
                                    \kappa_T^{(e)}\sigma_E^2\big)}{D^2}, }
\deqn{ \frac{\partial\,\mathrm{CCC}}{\partial \sigma_E^2}
      \;=\; -\,\frac{\kappa_T^{(e)}\,N}{D^2}, \qquad
       \frac{\partial\,\mathrm{CCC}}{\partial S_B}
      \;=\; -\,\frac{N}{D^2}. }

\emph{Estimating \eqn{\mathrm{Var}(\hat\theta)}.}
The EM updates write each variance component as an average of per-subject
quantities. For subject \eqn{i},
\deqn{ t_{A,i} \;=\; b_{i,0}^2 + (M_i^{-1})_{00},\qquad
       t_{M,i} \;=\; \frac{1}{nm}\sum_{\ell=1}^{nm}
                       \Big(b_{i,\ell}^2 + (M_i^{-1})_{\ell\ell}\Big), }
\deqn{ t_{T,i} \;=\; \frac{1}{nt}\sum_{j=1}^{nt}
                       \Big(b_{i,j}^2 + (M_i^{-1})_{jj}\Big),\qquad
       s_i \;=\; \frac{e_i^\top C_i(\rho)^{-1} e_i +
       \mathrm{tr}\!\big(M_i^{-1}U_i^\top C_i(\rho)^{-1} U_i\big)}{n_i}, }
where \eqn{b_i = M_i^{-1}(U_i^\top R_i^{-1} r_i)} and
\eqn{M_i = G^{-1} + U_i^\top R_i^{-1} U_i}.
With \eqn{m} subjects, form the empirical covariance of the stacked
subject vectors and scale by \eqn{m} to approximate the covariance of the
means:
\deqn{ \widehat{\mathrm{Cov}}\!\left(
      \begin{bmatrix} t_{A,\cdot} \\ t_{M,\cdot} \\ t_{T,\cdot} \end{bmatrix}
      \right)
      \;\approx\; \frac{1}{m}\,
       \mathrm{Cov}_i\!\left(
      \begin{bmatrix} t_{A,i} \\ t_{M,i} \\ t_{T,i} \end{bmatrix}\right). }
(Drop rows/columns as needed when \code{nm==0} or \code{nt==0}.)

The residual variance estimator is a weighted mean
\eqn{\hat\sigma_E^2=\sum_i w_i s_i} with \eqn{w_i=n_i/n}. Its variance is
approximated by the variance of a weighted mean of independent terms,
\deqn{ \widehat{\mathrm{Var}}(\hat\sigma_E^2)
      \;\approx\; \Big(\sum_i w_i^2\Big)\,\widehat{\mathrm{Var}}(s_i), }
where \eqn{\widehat{\mathrm{Var}}(s_i)} is the sample variance across
subjects. The method-dispersion term uses the quadratic-form delta already
computed for \eqn{S_B}:
\deqn{ \widehat{\mathrm{Var}}(S_B)
      \;=\; \frac{2\,\mathrm{tr}\!\big((A_{\!fix}\,\mathrm{Var}(\hat\beta))^2\big)
             \;+\; 4\,\hat\beta^\top A_{\!fix}\,\mathrm{Var}(\hat\beta)\,
             A_{\!fix}\,\hat\beta}
                   {\big[nm\,(nm-1)\,\max(nt,1)\big]^2}, }
with \eqn{A_{\!fix}=L\,D_m\,L^\top}.

\emph{Putting it together.} Assemble
\eqn{\widehat{\mathrm{Var}}(\hat\theta)} by combining the
\eqn{(\sigma_A^2,\sigma_{A\times M}^2,\sigma_{A\times T}^2)} covariance
block from the subject-level empirical covariance, add the
\eqn{\widehat{\mathrm{Var}}(\hat\sigma_E^2)} and
\eqn{\widehat{\mathrm{Var}}(S_B)} terms on the diagonal,
and ignore cross-covariances across these blocks (a standard large-sample
simplification). Then
\deqn{ \widehat{\mathrm{se}}\{\widehat{\mathrm{CCC}}\}
      \;=\; \sqrt{\,\nabla \mathrm{CCC}(\hat\theta)^\top\,
                    \widehat{\mathrm{Var}}(\hat\theta)\,
                    \nabla \mathrm{CCC}(\hat\theta)\,}. }

A two-sided \eqn{(1-\alpha)} normal CI is
\deqn{ \widehat{\mathrm{CCC}} \;\pm\; z_{1-\alpha/2}\,
      \widehat{\mathrm{se}}\{\widehat{\mathrm{CCC}}\}, }
truncated to \eqn{[0,1]} in the output for convenience. When \eqn{S_B} is
truncated at 0 or samples are very small/imbalanced, the normal CI can be
mildly anti-conservative near the boundary; a logit transform for CCC or a
subject-level (cluster) bootstrap can be used for sensitivity analysis.

\strong{Choosing \eqn{\rho} for AR(1).}
When \code{ar="ar1"} and \code{ar_rho = NA}, \eqn{\rho} is estimated by
profiling the REML log-likelihood at \eqn{(\hat\beta,\hat G,\hat\sigma_E^2)}.
With very few visits per subject, \eqn{\rho} can be weakly identified; consider
sensitivity checks over a plausible range.
}
\section{Notes on stability and performance}{

All per-subject solves are \eqn{\,r\times r} with \eqn{r=1+nm+nt+q_Z}, so cost
scales with the number of subjects and the fixed-effects dimension rather
than the total number of observations. Solvers use symmetric-PD paths with
a small diagonal ridge and pseudo-inverse,
which helps for very small/unbalanced subsets and near-boundary estimates.
For \code{AR(1)}, observations are ordered by time within subject; \code{NA} time codes
break the run, and gaps between factor levels are treated as regular steps
(elapsed time is not used).

\emph{Heteroscedastic slopes across \eqn{Z} columns are supported.}
Each \eqn{Z} column has its own variance component \eqn{\sigma_{Z,j}^2}, but
cross-covariances among \eqn{Z} columns are set to zero (diagonal block). Column
rescaling changes the implied prior on \eqn{b_{i,\text{extra}}} but does not
introduce correlations.
}

\examples{
# ====================================================================
# 1) Two methods (no time): baseline CCC
# ====================================================================
set.seed(1)
n_subj <- 30
meth   <- factor(rep(c("A","B"), each = n_subj))
id     <- factor(rep(seq_len(n_subj), times = 2))
sigA <- 1.0; sigE <- 0.5
u  <- rnorm(n_subj, 0, sqrt(sigA))
y  <- c(u + rnorm(n_subj, 0, sqrt(sigE)),
         u + 0.2 + rnorm(n_subj, 0, sqrt(sigE)))
dat <- data.frame(y, id, method = meth)

ccc_rm1 <- ccc_lmm_reml(dat, response = "y", rind = "id", method = "method")
print(ccc_rm1)
summary(ccc_rm1)

# 95\% CI container
ccc_rm2 <- ccc_lmm_reml(dat, response = "y", rind = "id", method = "method", ci = TRUE)
ccc_rm2

# ====================================================================
# 2) Subject x METHOD variance present, no time
#     y_{i,m} = mu + b_m + u_i + w_{i,m} + e_{i,m}
#     with u_i ~ N(0, s_A^2), w_{i,m} ~ N(0, s_{AxM}^2)
# ====================================================================
set.seed(102)
n_subj <- 60
n_time <- 8

id     <- factor(rep(seq_len(n_subj), each = 2 * n_time))
time   <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"),    each  = n_time), times = n_subj))

sigA  <- 0.6   # subject
sigAM <- 0.3   # subject × method
sigAT <- 0.5   # subject × time
sigE  <- 0.4   # residual
# Expected estimate S_B = 0.2^2 = 0.04
biasB <- 0.2   # fixed method bias

# random effects
u_i <- rnorm(n_subj, 0, sqrt(sigA))
u   <- u_i[as.integer(id)]

sm      <- interaction(id, method, drop = TRUE)
w_im_lv <- rnorm(nlevels(sm), 0, sqrt(sigAM))
w_im    <- w_im_lv[as.integer(sm)]

st      <- interaction(id, time, drop = TRUE)
g_it_lv <- rnorm(nlevels(st), 0, sqrt(sigAT))
g_it    <- g_it_lv[as.integer(st)]

# residuals & response
e <- rnorm(length(id), 0, sqrt(sigE))
y <- (method == "B") * biasB + u + w_im + g_it + e

dat_both <- data.frame(y, id, method, time)

# Both sigma2_subject_method and sigma2_subject_time are identifiable here
fit_both <- ccc_lmm_reml(dat_both, "y", "id", method = "method", time = "time",
                         vc_select = "auto", verbose = TRUE)
summary(fit_both)

# ====================================================================
# 3) Subject x TIME variance present (sag > 0) with two methods
#     y_{i,m,t} = mu + b_m + u_i + g_{i,t} + e_{i,m,t}
#     where g_{i,t} ~ N(0, s_{AxT}^2) shared across methods at time t
# ====================================================================
set.seed(202)
n_subj <- 60; n_time <- 14
id     <- factor(rep(seq_len(n_subj), each = 2 * n_time))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
time   <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))

sigA  <- 0.7
sigAT <- 0.5
sigE  <- 0.5
biasB <- 0.25

u   <- rnorm(n_subj, 0, sqrt(sigA))[as.integer(id)]
gIT <- rnorm(n_subj * n_time, 0, sqrt(sigAT))
g   <- gIT[ (as.integer(id) - 1L) * n_time + as.integer(time) ]
y   <- (method == "B") * biasB + u + g + rnorm(length(id), 0, sqrt(sigE))
dat_sag <- data.frame(y, id, method, time)

# sigma_AT should be retained; sigma_AM may be dropped (since w_{i,m}=0)
fit_sag <- ccc_lmm_reml(dat_sag, "y", "id", method = "method", time = "time",
                        vc_select = "auto", verbose = TRUE)
summary(fit_sag)

# ====================================================================
# 4) BOTH components present: sab > 0 and sag > 0 (2 methods x T times)
#     y_{i,m,t} = mu + b_m + u_i + w_{i,m} + g_{i,t} + e_{i,m,t}
# ====================================================================
set.seed(303)
n_subj <- 60; n_time <- 4
id     <- factor(rep(seq_len(n_subj), each = 2 * n_time))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
time   <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))

sigA  <- 0.8
sigAM <- 0.3
sigAT <- 0.4
sigE  <- 0.5
biasB <- 0.2

u   <- rnorm(n_subj, 0, sqrt(sigA))[as.integer(id)]
# (subject, method) random deviations: repeat per (i,m) across its times
wIM <- rnorm(n_subj * 2, 0, sqrt(sigAM))
w   <- wIM[ (as.integer(id) - 1L) * 2 + as.integer(method) ]
# (subject, time) random deviations: shared across methods at time t
gIT <- rnorm(n_subj * n_time, 0, sqrt(sigAT))
g   <- gIT[ (as.integer(id) - 1L) * n_time + as.integer(time) ]
y   <- (method == "B") * biasB + u + w + g + rnorm(length(id), 0, sqrt(sigE))
dat_both <- data.frame(y, id, method, time)

fit_both <- ccc_lmm_reml(dat_both, "y", "id", method = "method", time = "time",
                         vc_select = "auto", verbose = TRUE, ci = TRUE)
summary(fit_both)

# If you want to force-include both VCs (skip testing):
fit_both_forced <-
 ccc_lmm_reml(dat_both, "y", "id", method = "method", time = "time",
              vc_select = "none", include_subj_method  = TRUE,
              include_subj_time  = TRUE, verbose = TRUE)
summary(fit_both_forced)

# ====================================================================
# 5) D_m choices: time-averaged (default) vs typical visit
# ====================================================================
# Time-average
ccc_lmm_reml(dat_both, "y", "id", method = "method", time = "time",
             vc_select = "none", include_subj_method  = TRUE,
             include_subj_time  = TRUE, Dmat_type = "time-avg")

# Typical visit
ccc_lmm_reml(dat_both, "y", "id", method = "method", time = "time",
             vc_select = "none", include_subj_method  = TRUE,
             include_subj_time  = TRUE, Dmat_type = "typical-visit")

# ====================================================================
# 6) AR(1) residual correlation with fixed rho (larger example)
# ====================================================================
\donttest{
set.seed(10)
n_subj   <- 40
n_time   <- 10
methods  <- c("A", "B", "C", "D")
nm       <- length(methods)
id     <- factor(rep(seq_len(n_subj), each = n_time * nm))
method <- factor(rep(rep(methods, each = n_time), times = n_subj),
                 levels = methods)
time   <- factor(rep(rep(seq_len(n_time), times = nm), times = n_subj))

beta0    <- 0
beta_t   <- 0.2
bias_met <- c(A = 0.00, B = 0.30, C = -0.15, D = 0.05)
sigA     <- 1.0
rho_true <- 0.6
sigE     <- 0.7

t_num <- as.integer(time)
t_c   <- t_num - mean(seq_len(n_time))
mu    <- beta0 + beta_t * t_c + bias_met[as.character(method)]

u_subj <- rnorm(n_subj, 0, sqrt(sigA))
u      <- u_subj[as.integer(id)]

e <- numeric(length(id))
for (s in seq_len(n_subj)) {
  for (m in methods) {
    idx <- which(id == levels(id)[s] & method == m)
    e[idx] <- stats::arima.sim(list(ar = rho_true), n = n_time, sd = sigE)
  }
}
y <- mu + u + e
dat_ar4 <- data.frame(y = y, id = id, method = method, time = time)

ccc_lmm_reml(dat_ar4,
             response = "y", rind = "id", method = "method", time = "time",
             ar = "ar1", ar_rho = 0.6, verbose = TRUE)
}

# ====================================================================
# 7) Random slope variants (subject, method, custom Z)
# ====================================================================
\donttest{
## By SUBJECT
set.seed(2)
n_subj <- 60; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
subj <- as.integer(id)
slope_i <- rnorm(n_subj, 0, 0.15)
slope_vec <- slope_i[subj]
base <- rnorm(n_subj, 0, 1.0)[subj]
tnum <- as.integer(tim)
y <- base + 0.3*(method=="B") + slope_vec*(tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_s <- data.frame(y, id, method, time = tim)
dat_s$t_num <- as.integer(dat_s$time)
dat_s$t_c   <- ave(dat_s$t_num, dat_s$id, FUN = function(v) v - mean(v))
ccc_lmm_reml(dat_s, "y", "id", method = "method", time = "time",
             slope = "subject", slope_var = "t_c", verbose = TRUE)

## By METHOD
set.seed(3)
n_subj <- 60; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
slope_m <- ifelse(method=="B", 0.25, 0.00)
base <- rnorm(n_subj, 0, 1.0)[as.integer(id)]
tnum <- as.integer(tim)
y <- base + 0.3*(method=="B") + slope_m*(tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_m <- data.frame(y, id, method, time = tim)
dat_m$t_num <- as.integer(dat_m$time)
dat_m$t_c   <- ave(dat_m$t_num, dat_m$id, FUN = function(v) v - mean(v))
ccc_lmm_reml(dat_m, "y", "id", method = "method", time = "time",
             slope = "method", slope_var = "t_c", verbose = TRUE)

## SUBJECT + METHOD random slopes (custom Z)
set.seed(4)
n_subj <- 50; n_time <- 4
id  <- factor(rep(seq_len(n_subj), each = 2 * n_time))
tim <- factor(rep(rep(seq_len(n_time), times = 2), times = n_subj))
method <- factor(rep(rep(c("A","B"), each = n_time), times = n_subj))
subj <- as.integer(id)
slope_subj <- rnorm(n_subj, 0, 0.12)[subj]
slope_B    <- ifelse(method=="B", 0.18, 0.00)
tnum <- as.integer(tim)
base <- rnorm(n_subj, 0, 1.0)[subj]
y <- base + 0.3*(method=="B") +
     (slope_subj + slope_B) * (tnum - mean(seq_len(n_time))) +
     rnorm(length(id), 0, 0.5)
dat_bothRS <- data.frame(y, id, method, time = tim)
dat_bothRS$t_num <- as.integer(dat_bothRS$time)
dat_bothRS$t_c   <- ave(dat_bothRS$t_num, dat_bothRS$id, FUN = function(v) v - mean(v))
MM <- model.matrix(~ 0 + method, data = dat_bothRS)
Z_custom <- cbind(
  subj_slope = dat_bothRS$t_c,
  MM * dat_bothRS$t_c
)
ccc_lmm_reml(dat_bothRS, "y", "id", method = "method", time = "time",
             slope = "custom", slope_Z = Z_custom, verbose = TRUE)
}

}
\references{
Lin L (1989). A concordance correlation coefficient to evaluate reproducibility.
\emph{Biometrics}, 45: 255-268.

Lin L (2000). A note on the concordance correlation coefficient.
\emph{Biometrics}, 56: 324-325.

Carrasco, J. L. et al. (2013). Estimation of the concordance
correlation coefficient for repeated measures using SAS and R.
\emph{Computer Methods and Programs in Biomedicine}, 109(3), 293–304.
\doi{10.1016/j.cmpb.2012.09.002}

King et al. (2007). A Class of Repeated Measures Concordance
Correlation Coefficients.
\emph{Journal of Biopharmaceutical Statistics}, 17(4).
\doi{10.1080/10543400701329455}
}
\seealso{
\code{build_L_Dm_Z_cpp}
for constructing \eqn{L}/\eqn{D_m}/\eqn{Z}; \code{\link{ccc_pairwise_u_stat}}
for a U-statistic alternative; and \pkg{cccrm} for a reference approach via
\pkg{nlme}.
}
\author{
Thiago de Paula Oliveira
}
