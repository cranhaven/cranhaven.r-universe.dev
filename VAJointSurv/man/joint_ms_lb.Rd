% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/joint_surv_VA.R
\name{joint_ms_lb}
\alias{joint_ms_lb}
\alias{joint_ms_lb_gr}
\title{Evaluates the Lower Bound or the Gradient of the Lower Bound}
\usage{
joint_ms_lb(
  object,
  par,
  n_threads = object$max_threads,
  quad_rule = object$quad_rule,
  cache_expansions = object$cache_expansions,
  gh_quad_rule = object$gh_quad_rule
)

joint_ms_lb_gr(
  object,
  par,
  n_threads = object$max_threads,
  quad_rule = object$quad_rule,
  cache_expansions = object$cache_expansions,
  gh_quad_rule = object$gh_quad_rule
)
}
\arguments{
\item{object}{a joint_ms object from \code{\link{joint_ms_ptr}}.}

\item{par}{parameter vector for where the lower bound is evaluated at.}

\item{n_threads}{number of threads to use. This is not supported on Windows.}

\item{quad_rule}{list with nodes and weights for a quadrature rule for the
integral from zero to one.}

\item{cache_expansions}{\code{TRUE} if the expansions in the numerical
integration in the survival parts of the lower bound should be cached (not
recomputed). This requires more memory and may be an advantage
particularly with
expansions that take longer to compute (like \code{\link{ns_term}} and
\code{\link{bs_term}}). The computation time may be worse particularly if
you use more threads as the CPU cache is not well utilized.}

\item{gh_quad_rule}{list with two numeric vectors called node and weight
with Gaussâ€“Hermite quadrature nodes and weights to handle delayed entry.
A low number of quadrature nodes and weights is used when \code{NULL} is
passed.
This seems to work well when delayed entry happens at time with large
marginal survival probabilities. The nodes and weights can be obtained e.g.
from \code{fastGHQuad::gaussHermiteData}.}
}
\value{
\code{joint_ms_lb} returns a number scalar with the lower bound.

\code{joint_ms_lb_gr} returns a numeric vector with the gradient.
}
\description{
Evaluates the Lower Bound or the Gradient of the Lower Bound
}
\examples{
# load in the data
library(survival)
data(pbc, package = "survival")

# re-scale by year
pbcseq <- transform(pbcseq, day_use = day / 365.25)
pbc <- transform(pbc, time_use = time / 365.25)

# create the marker terms
m1 <- marker_term(
  log(bili) ~ 1, id = id, data = pbcseq,
  time_fixef = bs_term(day_use, df = 5L),
  time_rng = poly_term(day_use, degree = 1L, raw = TRUE, intercept = TRUE))
m2 <- marker_term(
  albumin ~ 1, id = id, data = pbcseq,
  time_fixef = bs_term(day_use, df = 5L),
  time_rng = poly_term(day_use, degree = 1L, raw = TRUE, intercept = TRUE))

# base knots on observed event times
bs_term_knots <-
  with(pbc, quantile(time_use[status == 2], probs = seq(0, 1, by = .2)))

boundary <- c(bs_term_knots[ c(1, length(bs_term_knots))])
interior <- c(bs_term_knots[-c(1, length(bs_term_knots))])

# create the survival term
s_term <- surv_term(
  Surv(time_use, status == 2) ~ 1, id = id, data = pbc,
  time_fixef = bs_term(time_use, Boundary.knots = boundary, knots = interior))

# create the C++ object to do the fitting
model_ptr <- joint_ms_ptr(
  markers = list(m1, m2), survival_terms = s_term,
  max_threads = 2L, ders = list(0L, c(0L, -1L)))

# find the starting values
start_vals <- joint_ms_start_val(model_ptr)

# same lower bound
all.equal(attr(start_vals,"value"),joint_ms_lb(model_ptr,par = start_vals))
}
